# Local LLM Configuration
# Uncomment and modify these settings based on your setup

# Server type: ollama, llamacpp, or huggingface
LLM_SERVER_TYPE=ollama

# Base URL for your LLM server
# Ollama: http://localhost:11434
# llama.cpp: http://localhost:8080  
# HuggingFace: http://localhost:5000
LLM_BASE_URL=http://localhost:11434

# Model name to use
# Ollama examples: llama3.2, codellama, mistral
# llama.cpp: depends on your loaded model
# HuggingFace: microsoft/DialoGPT-medium, meta-llama/Llama-2-7b-chat-hf
LLM_MODEL=llama3.2

# LLM generation parameters
LLM_TIMEOUT=30
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=1000

# Enable/disable LLM enhancement (true/false)
ENABLE_LLM=true
